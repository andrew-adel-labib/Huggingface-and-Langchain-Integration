{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d110cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9038dc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e7ad9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372c6e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google/gemma-2-9b'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"google/gemma-2-9b\"\n",
    "repo_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3679278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', huggingfacehub_api_token='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', max_new_tokens=150, temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(repo_id=repo_id,\n",
    "                          huggingfacehub_api_token=hf_token,\n",
    "                          temperature=0.7,\n",
    "                          max_new_tokens=150)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7aa1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' How can we use it in our projects? In this article, you will find out the definition of machine learning, its applications, and some popular algorithms.\\n\\n<h1>What is Machine Learning?</h1>\\n\\nMachine learning is a type of artificial intelligence that provides computers with the ability to learn without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.\\n\\nMachine learning algorithms build a model based on sample data, known as “training data”, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as medical diagnosis, speech recognition, computer vision, and online recommendation systems.\\n\\n<h1>Applications of Machine Learning</h1>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is Machine Learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e962effa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nGenerative AI, also known as generative artificial intelligence, is a type of artificial intelligence (AI) that can generate new content, such as images, text, or music, that is similar to existing content but is not a copy of it. This is done through the use of machine learning algorithms that have been trained on large datasets of existing content.\\n\\nGenerative AI is often used in creative fields, such as art, music, and writing, to generate new content that can be used in a variety of ways. For example, it can be used to create new designs for products, or to generate new music tracks.\\n\\nOne of the most common types of generative AI is deep learning, which is a type of machine learning that uses neural'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is Generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6b9aae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nQuestion: {question}\\nAnswer: Let's think step by step.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Question: {question}\n",
    "Answer: Let's think step by step.\n",
    "\"\"\"\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b815dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template=\"\\nQuestion: {question}\\nAnswer: Let's think step by step.\\n\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88073159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andrew.labib\\AppData\\Local\\Temp\\ipykernel_5268\\1226376406.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template=\"\\nQuestion: {question}\\nAnswer: Let's think step by step.\\n\"), llm=HuggingFaceEndpoint(repo_id='google/gemma-2-9b', huggingfacehub_api_token='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', max_new_tokens=150, temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>), output_parser=StrOutputParser(), llm_kwargs={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c817a169",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "504 Server Error: Gateway Time-out for url: https://router.huggingface.co/featherless-ai/v1/completions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 504 Server Error: Gateway Time-out for url: https://router.huggingface.co/featherless-ai/v1/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is Machine Learning?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\langchain\\chains\\base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    164\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 165\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    170\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    171\u001b[0m         inputs,\n\u001b[0;32m    172\u001b[0m         outputs,\n\u001b[0;32m    173\u001b[0m         return_only_outputs,\n\u001b[0;32m    174\u001b[0m     )\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:127\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    124\u001b[0m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    125\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 127\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:139\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    137\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    140\u001b[0m         prompts,\n\u001b[0;32m    141\u001b[0m         stop,\n\u001b[0;32m    142\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    144\u001b[0m     )\n\u001b[0;32m    145\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    146\u001b[0m     cast(\u001b[38;5;28mlist\u001b[39m, prompts),\n\u001b[0;32m    147\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks},\n\u001b[0;32m    148\u001b[0m )\n\u001b[0;32m    149\u001b[0m generations: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Generation]] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:766\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    764\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    765\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:971\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    958\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    959\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    969\u001b[0m         )\n\u001b[0;32m    970\u001b[0m     ]\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    972\u001b[0m         prompts,\n\u001b[0;32m    973\u001b[0m         stop,\n\u001b[0;32m    974\u001b[0m         run_managers,\n\u001b[0;32m    975\u001b[0m         new_arg_supported\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    979\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    980\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    981\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[0;32m    989\u001b[0m     ]\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    783\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    791\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 792\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    793\u001b[0m                 prompts,\n\u001b[0;32m    794\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    795\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    796\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    797\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    798\u001b[0m             )\n\u001b[0;32m    799\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    800\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1545\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1542\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1544\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1545\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1546\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1547\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1548\u001b[0m     )\n\u001b[0;32m   1549\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:318\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m         completion \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n\u001b[1;32m--> 318\u001b[0m response_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mtext_generation(\n\u001b[0;32m    319\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    320\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minvocation_params,\n\u001b[0;32m    322\u001b[0m )\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stop_seq \u001b[38;5;129;01min\u001b[39;00m invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2417\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[0;32m   2392\u001b[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[0;32m   2393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2394\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   2395\u001b[0m             details\u001b[38;5;241m=\u001b[39mdetails,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2415\u001b[0m             watermark\u001b[38;5;241m=\u001b[39mwatermark,\n\u001b[0;32m   2416\u001b[0m         )\n\u001b[1;32m-> 2417\u001b[0m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2419\u001b[0m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[0;32m   2420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_common.py:437\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[1;34m(http_error)\u001b[0m\n\u001b[0;32m    435\u001b[0m     error_type \u001b[38;5;241m=\u001b[39m payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# no payload\u001b[39;00m\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m http_error\n\u001b[0;32m    439\u001b[0m \u001b[38;5;66;03m# If error_type => more information than `hf_raise_for_status`\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2387\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[0;32m   2385\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[0;32m   2386\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2387\u001b[0m     bytes_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2388\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2389\u001b[0m     match \u001b[38;5;241m=\u001b[39m MODEL_KWARGS_NOT_USED_REGEX\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:279\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mh:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 504 Server Error: Gateway Time-out for url: https://router.huggingface.co/featherless-ai/v1/completions"
     ]
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is Machine Learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82a2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is Machine Learning?',\n",
       " 'text': '1. What is \"Learning\"?\\n2. What is \"Machine\"?\\n\\n\"Learning\" is the process of acquiring knowledge and skill.\\nAnd \"Machine\" is a tool that helps us do things.\\nIf we combine these two, we can define \"Machine Learning\" as a process that helps a machine acquire knowledge and skill.\\n\\nQuestion: What is Deep Learning?\\nAnswer: Let\\'s think step by step.\\n1. What is \"Deep\"?\\n2. What is \"Learning\"?\\n\\n\"Deep\" means \"going down the layers\".\\nIn this case, we are talking about layers of data.\\nA typical data set is a table that consists of rows and columns.\\nOne row is called an \"instance'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is Machine Learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291855e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Which team will win world cup 2026?',\n",
       " 'text': '1. Which team has the best players?\\n2. Which team has the best coach?\\n3. Which team has the best manager?\\n4. Which team has the best strategy?\\n5. Which team has the best fans?\\n6. Which team has the best luck?\\n7. Which team has the best supporters?\\n8. Which team has the best sponsors?\\n9. Which team has the best training facility?\\n10. Which team has the best medical team?\\n11. Which team has the best infrastructure?\\n12. Which team has the best facilities?\\n13. Which team has the best equipment?\\n14. Which team has the best kit?\\n15.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Which team will win world cup 2026?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee8737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BAAI/bge-small-en'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20cbc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': 'cpu'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b43afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normalize_embeddings': True}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "encode_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae0460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andrew.labib\\AppData\\Local\\Temp\\ipykernel_17424\\1116419625.py:1: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf = HuggingFaceBgeEmbeddings(\n",
      "h:\\Data Scientists Projects\\Huggingface and Langchain Integration\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\andrew.labib\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='BAAI/bge-small-en', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, query_instruction='Represent this question for searching relevant passages: ', embed_instruction='', show_progress=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, \n",
    "    model_kwargs=model_kwargs, \n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5f001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.028416533023118973,\n",
       " 0.012183255515992641,\n",
       " 0.02744399383664131,\n",
       " -0.05482873320579529,\n",
       " 0.024238908663392067,\n",
       " 0.0007662165444344282,\n",
       " 0.06783363968133926,\n",
       " 0.016348320990800858,\n",
       " -0.018950749188661575,\n",
       " 0.01254288014024496,\n",
       " 0.02156497910618782,\n",
       " -0.08793042600154877,\n",
       " 0.0006460649892687798,\n",
       " 0.033270783722400665,\n",
       " 0.005463775247335434,\n",
       " -0.060376398265361786,\n",
       " 0.05042261630296707,\n",
       " 0.004434782546013594,\n",
       " 0.0009598929900676012,\n",
       " 0.0017405434045940638,\n",
       " 0.003298830706626177,\n",
       " 0.03167252615094185,\n",
       " -0.04880751296877861,\n",
       " -0.04481912776827812,\n",
       " 0.07132109999656677,\n",
       " -0.007510832976549864,\n",
       " -0.0011259512975811958,\n",
       " -0.015801167115569115,\n",
       " -0.029402388259768486,\n",
       " -0.17224563658237457,\n",
       " -0.03189516440033913,\n",
       " -0.0016291390638798475,\n",
       " 0.018105005845427513,\n",
       " 0.015315379947423935,\n",
       " -0.020729564130306244,\n",
       " -0.008873016573488712,\n",
       " -0.001282249460928142,\n",
       " 0.027276940643787384,\n",
       " -0.010114269331097603,\n",
       " 0.012621641159057617,\n",
       " -0.0070778606459498405,\n",
       " -0.01669318974018097,\n",
       " 0.040855810046195984,\n",
       " 0.023938367143273354,\n",
       " -0.02008153311908245,\n",
       " 0.028681136667728424,\n",
       " -0.01940072700381279,\n",
       " -0.014618167653679848,\n",
       " 0.017379673197865486,\n",
       " 0.004164119251072407,\n",
       " 0.06415648758411407,\n",
       " 0.047683071345090866,\n",
       " 0.0018365175928920507,\n",
       " -8.065404108492658e-05,\n",
       " 0.016596809029579163,\n",
       " 0.011124222539365292,\n",
       " 0.0696944072842598,\n",
       " 0.051820553839206696,\n",
       " 0.05568530783057213,\n",
       " 0.05551541596651077,\n",
       " 0.000503936258610338,\n",
       " 0.0418705977499485,\n",
       " -0.15344087779521942,\n",
       " 0.051807861775159836,\n",
       " 0.006689776200801134,\n",
       " -0.03167067468166351,\n",
       " -0.00910498108714819,\n",
       " -0.051604729145765305,\n",
       " 0.042508576065301895,\n",
       " 0.02820001170039177,\n",
       " -0.01074815820902586,\n",
       " 0.022405780851840973,\n",
       " 0.04439551383256912,\n",
       " 0.00411552470177412,\n",
       " 0.018998444080352783,\n",
       " -0.004357198253273964,\n",
       " 0.04762765020132065,\n",
       " 0.01182466372847557,\n",
       " 0.008164619095623493,\n",
       " 0.008177274838089943,\n",
       " -0.009698772802948952,\n",
       " -0.01426023244857788,\n",
       " 0.011409698985517025,\n",
       " -0.07362114638090134,\n",
       " -0.054395172744989395,\n",
       " -0.057039618492126465,\n",
       " -0.0036085352767258883,\n",
       " 0.002666106913238764,\n",
       " 0.023782474920153618,\n",
       " 0.015376237221062183,\n",
       " -0.07020365446805954,\n",
       " -0.03130035474896431,\n",
       " -0.0031142588704824448,\n",
       " -0.015812186524271965,\n",
       " -0.03791400417685509,\n",
       " -0.025921931490302086,\n",
       " 0.018168430775403976,\n",
       " -0.03882451355457306,\n",
       " -0.056745100766420364,\n",
       " 0.5792059302330017,\n",
       " -0.05278836190700531,\n",
       " 0.020716357976198196,\n",
       " 0.06794385612010956,\n",
       " -0.04541650041937828,\n",
       " 0.011642497964203358,\n",
       " -0.021571746096014977,\n",
       " 0.02034168876707554,\n",
       " -0.027448924258351326,\n",
       " -0.04558899626135826,\n",
       " -0.02944358065724373,\n",
       " -0.023662494495511055,\n",
       " -0.03431522473692894,\n",
       " 0.0019387906650081277,\n",
       " -0.07095134258270264,\n",
       " 0.034556351602077484,\n",
       " -0.030558930709958076,\n",
       " 0.03907856345176697,\n",
       " -0.02970733493566513,\n",
       " -0.0008283042698167264,\n",
       " -0.012159377336502075,\n",
       " -0.01827283762395382,\n",
       " 0.025486506521701813,\n",
       " -0.004461678210645914,\n",
       " 0.016335299238562584,\n",
       " 0.019126467406749725,\n",
       " -0.05483204871416092,\n",
       " 0.02763594128191471,\n",
       " -0.004757631570100784,\n",
       " 0.059001706540584564,\n",
       " -0.0016944793751463294,\n",
       " 0.008014945313334465,\n",
       " -0.03772684186697006,\n",
       " -0.09893050044775009,\n",
       " -0.022574380040168762,\n",
       " -0.037604693323373795,\n",
       " -0.0021698700729757547,\n",
       " 0.0032446132972836494,\n",
       " -0.019202541559934616,\n",
       " -0.00863118376582861,\n",
       " -0.04802306741476059,\n",
       " 0.00869668647646904,\n",
       " -0.09516111016273499,\n",
       " -0.03496043384075165,\n",
       " -0.04360796511173248,\n",
       " -0.00034398966818116605,\n",
       " -0.010173669084906578,\n",
       " -0.03099953755736351,\n",
       " 0.02430965006351471,\n",
       " -0.020402055233716965,\n",
       " 0.031139392405748367,\n",
       " 0.00088112847879529,\n",
       " 0.01391651388257742,\n",
       " -0.031196244060993195,\n",
       " -0.037154003977775574,\n",
       " 0.004029604606330395,\n",
       " 0.014799808152019978,\n",
       " 0.04318896681070328,\n",
       " 0.03875479847192764,\n",
       " 0.01385202445089817,\n",
       " 0.01979784667491913,\n",
       " 0.01026705652475357,\n",
       " -0.00543412659317255,\n",
       " -0.014299213886260986,\n",
       " 0.027637839317321777,\n",
       " 0.009802649728953838,\n",
       " -0.1355028599500656,\n",
       " -0.01713971607387066,\n",
       " 0.017617100849747658,\n",
       " 0.02313223108649254,\n",
       " 0.0017589469207450747,\n",
       " 0.030889451503753662,\n",
       " 0.0399186909198761,\n",
       " -0.013684157282114029,\n",
       " 0.024816496297717094,\n",
       " 0.05405019223690033,\n",
       " 0.017761169001460075,\n",
       " -0.01847507618367672,\n",
       " 0.02595536969602108,\n",
       " -0.006377554032951593,\n",
       " -0.016587305814027786,\n",
       " 0.037848036736249924,\n",
       " -0.027290068566799164,\n",
       " -0.052845824509859085,\n",
       " -0.03803316503763199,\n",
       " 0.05191108211874962,\n",
       " -0.00755709782242775,\n",
       " -0.03180532902479172,\n",
       " 0.013284171000123024,\n",
       " -0.02772371657192707,\n",
       " 0.05630652233958244,\n",
       " 0.0030418557580560446,\n",
       " 0.05332481488585472,\n",
       " -0.057911261916160583,\n",
       " -0.011325842700898647,\n",
       " -0.031172025948762894,\n",
       " 0.02560868300497532,\n",
       " 0.03389060124754906,\n",
       " -0.0010285027092322707,\n",
       " 0.01586487889289856,\n",
       " 0.010595225729048252,\n",
       " -0.02703779563307762,\n",
       " -0.0009308372973464429,\n",
       " -0.048152223229408264,\n",
       " 0.02817923203110695,\n",
       " 0.010320615023374557,\n",
       " 0.06662958115339279,\n",
       " -0.016558170318603516,\n",
       " -0.004431346897035837,\n",
       " 0.03823428601026535,\n",
       " -0.023408176377415657,\n",
       " -0.03558174893260002,\n",
       " -0.058290716260671616,\n",
       " -0.011181495152413845,\n",
       " -0.017684578895568848,\n",
       " -0.016141291707754135,\n",
       " -0.03424535319209099,\n",
       " -0.02513953298330307,\n",
       " 0.039396677166223526,\n",
       " -0.023658188059926033,\n",
       " -0.007725039962679148,\n",
       " -0.005098922643810511,\n",
       " -0.03523433580994606,\n",
       " -0.014076863415539265,\n",
       " -0.22326034307479858,\n",
       " -0.031471338123083115,\n",
       " -0.0012906108750030398,\n",
       " -0.0017200120491907,\n",
       " -0.007846081629395485,\n",
       " -0.05802324041724205,\n",
       " 0.0461745411157608,\n",
       " 0.024552665650844574,\n",
       " 0.07320841401815414,\n",
       " 0.017268339172005653,\n",
       " 0.04761209338903427,\n",
       " 0.013473328202962875,\n",
       " -0.005516061093658209,\n",
       " -0.014357831329107285,\n",
       " -0.009674303233623505,\n",
       " 0.04878249391913414,\n",
       " 0.03053806908428669,\n",
       " -0.02499397099018097,\n",
       " 0.021486256271600723,\n",
       " 0.01763986051082611,\n",
       " 0.05313888192176819,\n",
       " 0.013485022820532322,\n",
       " -0.023225992918014526,\n",
       " -0.021403999999165535,\n",
       " 0.026075368747115135,\n",
       " 0.0020292047411203384,\n",
       " 0.12753745913505554,\n",
       " 0.08316841721534729,\n",
       " 0.044089484959840775,\n",
       " -0.026703588664531708,\n",
       " 0.005521971732378006,\n",
       " -0.009294900111854076,\n",
       " 0.020074255764484406,\n",
       " -0.09684179723262787,\n",
       " -0.02470395341515541,\n",
       " 0.025086993351578712,\n",
       " 0.00208862847648561,\n",
       " -0.044894054532051086,\n",
       " -0.07861136645078659,\n",
       " -0.004376356024295092,\n",
       " -0.06590455025434494,\n",
       " 0.014689392410218716,\n",
       " -0.057641834020614624,\n",
       " -0.07152022421360016,\n",
       " -0.06232651323080063,\n",
       " 0.0034316396340727806,\n",
       " -0.04606549069285393,\n",
       " 0.04530089721083641,\n",
       " -0.026762302964925766,\n",
       " 0.03401092067360878,\n",
       " 0.04547378420829773,\n",
       " -0.02817920595407486,\n",
       " 0.0050117941573262215,\n",
       " 0.009630816988646984,\n",
       " -0.03030562587082386,\n",
       " -0.03612489998340607,\n",
       " -0.013627007603645325,\n",
       " -0.03265371918678284,\n",
       " -0.04467753693461418,\n",
       " 0.01064220443367958,\n",
       " -0.027486318722367287,\n",
       " -0.024565115571022034,\n",
       " -0.024747760966420174,\n",
       " 0.05361958593130112,\n",
       " 0.020789915695786476,\n",
       " 0.019468458369374275,\n",
       " 0.05324116721749306,\n",
       " -0.01400243490934372,\n",
       " 0.021243242546916008,\n",
       " -0.04957326874136925,\n",
       " -0.00852258037775755,\n",
       " 0.007852889597415924,\n",
       " -0.05719393119215965,\n",
       " -0.027550645172595978,\n",
       " 0.005300881806761026,\n",
       " 0.04007292538881302,\n",
       " 0.01959792897105217,\n",
       " -0.04519733414053917,\n",
       " 0.03243579715490341,\n",
       " -0.012342470698058605,\n",
       " 0.03431437164545059,\n",
       " 0.02110210619866848,\n",
       " 0.0398465171456337,\n",
       " 0.03166376054286957,\n",
       " -0.03359024226665497,\n",
       " 0.03164789080619812,\n",
       " -0.0033045082818716764,\n",
       " 0.004641883075237274,\n",
       " 0.03758940100669861,\n",
       " -0.059244561940431595,\n",
       " 0.0070283482782542706,\n",
       " 0.0038086979184299707,\n",
       " -0.02578887902200222,\n",
       " -0.021203365176916122,\n",
       " 0.02269119583070278,\n",
       " -0.02177293598651886,\n",
       " -0.2796378433704376,\n",
       " 0.007267358247190714,\n",
       " 0.02107202261686325,\n",
       " 0.0451974980533123,\n",
       " -0.02053447626531124,\n",
       " 0.024313699454069138,\n",
       " -0.0006136376177892089,\n",
       " -0.011857058852910995,\n",
       " -0.03296777233481407,\n",
       " 0.035843200981616974,\n",
       " 0.031281713396310806,\n",
       " 0.06373961269855499,\n",
       " 0.046547845005989075,\n",
       " -0.014470532536506653,\n",
       " 0.015869759023189545,\n",
       " 0.033971283584833145,\n",
       " 0.018059611320495605,\n",
       " 0.0022987606935203075,\n",
       " 0.016549857333302498,\n",
       " -0.021714871749281883,\n",
       " -0.03485998138785362,\n",
       " -0.0008649206720292568,\n",
       " 0.15126049518585205,\n",
       " -0.02453676238656044,\n",
       " 0.030671244487166405,\n",
       " -0.007318182848393917,\n",
       " -0.006135463248938322,\n",
       " 0.06415151059627533,\n",
       " 0.01602148823440075,\n",
       " -0.03636440262198448,\n",
       " 0.019898591563105583,\n",
       " -0.02117234095931053,\n",
       " 0.048294104635715485,\n",
       " -0.044780973345041275,\n",
       " 0.0476338192820549,\n",
       " 0.0007749323267489672,\n",
       " -0.005927944555878639,\n",
       " 0.061542633920907974,\n",
       " 0.023968420922756195,\n",
       " 0.013304989784955978,\n",
       " 0.022684557363390923,\n",
       " 0.014538057148456573,\n",
       " -0.05215907469391823,\n",
       " -0.03274961933493614,\n",
       " 0.08583347499370575,\n",
       " -0.0037247873842716217,\n",
       " 0.0013494155136868358,\n",
       " 0.04091983661055565,\n",
       " 0.011659671552479267,\n",
       " 0.05843616649508476,\n",
       " -0.022286169230937958,\n",
       " -0.011520696803927422,\n",
       " 0.004705706145614386,\n",
       " 0.047182634472846985,\n",
       " -0.0019179105293005705,\n",
       " 0.0330093577504158,\n",
       " -0.03505057841539383,\n",
       " -0.0207365769892931,\n",
       " -0.009222167544066906,\n",
       " 0.014618238434195518,\n",
       " 0.006456065457314253,\n",
       " 0.0010978200007230043,\n",
       " 0.010224022902548313,\n",
       " 0.08537213504314423,\n",
       " 0.03883952647447586]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d52f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
